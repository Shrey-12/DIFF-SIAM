{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, bonafide_dir, morphed_dir, transform=None):\n",
        "        self.bonafide_dir = bonafide_dir\n",
        "        self.morphed_dir = morphed_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create lists of file paths\n",
        "        self.bonafide_files = [os.path.join(bonafide_dir, f) for f in os.listdir(bonafide_dir)]\n",
        "        self.morphed_files = [os.path.join(morphed_dir, f) for f in os.listdir(morphed_dir)]\n",
        "\n",
        "        # Prepare file indices\n",
        "        self.bonafide_indices = list(range(len(self.bonafide_files)))\n",
        "        self.morphed_indices = list(range(len(self.morphed_files)))\n",
        "\n",
        "        # Split data into train and test\n",
        "        self.train_bonafide_files, self.test_bonafide_files = train_test_split(\n",
        "            self.bonafide_files, test_size=0.2, random_state=42\n",
        "        )\n",
        "        self.train_morphed_files, self.test_morphed_files = train_test_split(\n",
        "            self.morphed_files, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_bonafide_files) * 2  # Increase size for more triplets\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if idx % 2 == 0:\n",
        "          # Anchor and Positive: both from bonafide\n",
        "          anchor_idx = idx % len(self.train_bonafide_files)\n",
        "          positive_idx = (anchor_idx + 1) % len(self.train_bonafide_files)\n",
        "          anchor_path = self.train_bonafide_files[anchor_idx]\n",
        "          positive_path = self.train_bonafide_files[positive_idx]\n",
        "          negative_path = np.random.choice(self.train_morphed_files)\n",
        "      else:\n",
        "          # Anchor and Positive: both from morphed\n",
        "          anchor_idx = idx % len(self.train_morphed_files)\n",
        "          positive_idx = (anchor_idx + 1) % len(self.train_morphed_files)\n",
        "          anchor_path = self.train_morphed_files[anchor_idx]\n",
        "          positive_path = self.train_morphed_files[positive_idx]\n",
        "          negative_path = np.random.choice(self.train_bonafide_files)\n",
        "\n",
        "      # Load images\n",
        "      anchor = self.load_image(anchor_path)\n",
        "      anchor_name = os.path.basename(anchor_path)\n",
        "      positive = self.load_image(positive_path)\n",
        "      negative = self.load_image(negative_path)\n",
        "\n",
        "      return anchor, positive, negative, anchor_name\n",
        "\n",
        "\n",
        "    def load_image(self, path):\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n"
      ],
      "metadata": {
        "id": "96SyUtXUmi4b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c6rxPHIHFgSK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.inception = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256*3*3, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 128)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.inception(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        output3 = self.forward_one(input3)\n",
        "        return output1, output2, output3\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_dist = F.pairwise_distance(anchor, positive, 2)\n",
        "        neg_dist = F.pairwise_distance(anchor, negative, 2)\n",
        "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
        "        return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to a suitable size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "l60BjzEuo_y_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "loss_dict = {}\n",
        "\n",
        "def train_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = SiameseNetwork().to(device)\n",
        "    criterion = TripletLoss(margin=1.0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    dataset = TripletDataset(\n",
        "        bonafide_dir='path_to_CASIA_Webface',\n",
        "        morphed_dir='path_to_FRLL',\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for anchor, positive, negative, anchor_name in dataloader:\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output1, output2, output3 = model(anchor, positive, negative)\n",
        "            loss = criterion(output1, output2, output3)\n",
        "            loss_dict[anchor_name] = loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n",
        "\n",
        "with open('loss_data.json', 'r') as f:\n",
        "    loss_dict = json.load(f)"
      ],
      "metadata": {
        "id": "wT8onPhPFn-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "yW8Ei5Y-stnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import device\n",
        "\n",
        "# Load the trained model\n",
        "model = SiameseNetwork().to(device)  # Ensure this matches your trained model structure\n",
        "model.load_state_dict(torch.load('path_to_trained_model.pth'))  # Load the model weights\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "loss_dict = {}\n",
        "\n",
        "# Define your loss function\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "# Use the existing TripletDataset class for testing\n",
        "test_dataset = TripletDataset(\n",
        "    bonafide_dir='path_to_CASIA_Webface_features',\n",
        "    morphed_dir='path_to_FRLL_features'\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, data_loader, criterion, device):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for anchor, positive, negative,anchor_name in data_loader:\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output1, output2, output3 = model(anchor, positive, negative)\n",
        "            loss = criterion(output1, output2, output3)\n",
        "            loss_dict[anchor_name] = loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(data_loader)\n",
        "    print(f'Average Test Loss: {average_loss}')\n",
        "\n",
        "# Run the test\n",
        "test_model(model, test_loader, criterion, device)\n",
        "\n",
        "with open('testing_loss_data.json', 'r') as f:\n",
        "    loss_dict = json.load(f)\n"
      ],
      "metadata": {
        "id": "aQo14h9XssJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}